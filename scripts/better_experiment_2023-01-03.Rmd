---
title: "Quicksort performance analysis"
author: "Ilya Meignan--Masson and Nour El hassane"
date: "2022-11-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(ggplot2)
get_hostname <- function(){
  return(gsub(" ", "", as.character(Sys.info()["nodename"])))
}
```
# Problem statement
Improve the experimental design and the performance evaluation of a simple parallel quicksort implementation. This would be done by taking into account the replication and randomization concepts, so that we have reliable unbiaised evaluations.

# Experimental design

## Generate the arrays of parameters (number of execution repetitions and different array sizes)
```{r}
number_of_repetitions <- c(0:10)
sizes <- c("100","500", "1000","5000", "10000","50000", "100000")
```
## Generate the combinations
```{r}
experiments <- crossing(number_of_repetitions, sizes) #pseudo-replication
experiments <- experiments[sample(1:nrow(experiments)), ]#randomization
experiments
```
## Run the functions
```{r}
# store the experiments parameters in a csv file
write.csv(experiments, "../data/nour_2023-01-03/experiments.csv", row.names=FALSE)
```

Then, from the project directory /M2R-ParallelQuicksort, execute the run_benchmarking.sh script (located in scripts folder).
```{bash, engine.opts='-l'}
cd /home/nour/projects/ENSIMAG/SMPE/M2R-ParallelQuicksort
./scripts/run_benchmarking.sh
```

Once experiments are run, we convert the measurements.txt file to a measurements.csv file to perfom data analysis on it.
```{bash, engine.opts='-l'}
cd /home/nour/projects/ENSIMAG/SMPE/M2R-ParallelQuicksort
./scripts/csv_quicksort_extractor.sh
```

# Analysis of the results

```{r}
df <- read.csv(file="../data/nour_2023-01-03/measurements.csv", sep=",",header = FALSE)
names(df) <- c('Size','Function','Execution_time')
summary(df)
```
Calculate the mean execution time (sample mean) for each function. Function 0 is the sequential sort, 1 is the parallel quicksort, and 2 is the Built-in sort.
```{r}
sample_means <- df %>% group_by(Function) %>% summarise(Sn=mean(`Execution_time`))
sample_means
```
Wow!! It seems that the parallel quicksort is the worst algorithm!
Let's use the boxplot and confirm that using the **Confidence Interval**.
```{r}
ggplot(data=df,aes(x=Function,y=Execution_time, group=factor(Function)))+ geom_boxplot() + theme_bw()
```

```{r}
min_1 <-min(df$Execution_time[df$Function==1])
max_0 <- max(df$Execution_time[df$Function==0])
max_2 <- max(df$Execution_time[df$Function==2])
print(paste(min_1,max_0,max_2))
```
Min execution time of the parallel quicksort is less than max sequential and max built-in. Thus, intervals overlap!
Compare Var(1-0) to Var(1) and Var(0), and repeat the same for Var(1-2) comapred to Var(1) and Var(2).
```{r}
df_0<-df$Execution_time[df$Function==0]
df_1<-df$Execution_time[df$Function==1]
df_2<-df$Execution_time[df$Function==2]
cat("Var(1-0): ",var(df_1-df_0))
cat("\nVar(1): ",var(df_1))
cat("\nVar(0): ",var(df_0))
```
Var(1-0) is smaller than Var(1) and Var(0), so we can conclude that \mu_0 < \mu_1 with 95% of confidence.
```{r}
cat("Var(1-2): ",var(df_1-df_2))
cat("\nVar(1): ",var(df_1))
cat("\nVar(2): ",var(df_2))
```
Var(1-2) is smaller than Var(1) and Var(2) in this version, so we can conclude that \mu_2 < \mu_1 with 95% of confidence.

Therefore, we can say that through this experimental analysis, it appeared that the Built-in quicksort function is better than the parallel quicksort function.

Note that in a previous experiment, Var(1-2) was smaller than Var(1) but bigger than Var(2), so we considered the Variable 1-2 and did a boxplot. We found that E[1-2]>0 => E[1] > E[2], which means that C quicksort is better than the parallel quicksort.